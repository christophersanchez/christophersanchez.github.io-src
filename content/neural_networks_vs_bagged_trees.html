<html>
    <head>
        :url: override/url/
        :save_as: override/url/
        <font color="fff"><title>Neural Networks Vs Bagged Trees</title></font>
        <meta name="tags" content="data science, neural network, sklearn, python, bagged trees, numpy, pandas, matplotlib" />
        <meta name="date" content="2018 - 07 - 24" />
        <meta name="category" content="Machine Learning" />
        <meta name="authors" content="Chris Sanchez" />
        <meta name="summary" content="A comparison between SKlearns MLPClassifier and SKlearns Bagged Trees." />

    </head>
    <body>
    <font color="#32cd32"><p>  Hello, thanks for stopping by. Today we have a few goals that we are going to accomplish. We will build a multi-layer perceptron neural network model to predict on a labeled dataset that we will find online. We will build the best model we can, by diving into the hyperparameters and discovering the best combination for our data. We will then compare our perceptron model to a boosted tree model to examine the efficiencies of both, and determine which model better solves our output question. We will check for complexity and accuracy.<br />
          </p>

      <ul><strong> Our checklist:</strong>
        <li>Find a dataset</li>
        <li>Introduce our dataset</li>
        <li>Import our necessary modules, and the data</li>
        <li>Examine the data to get a better understanding of our data</li>
        <li>Clean and process the data</li>
        <li>Build our models</li>
        <li>Conclude by comparing and contrasting our two models.</li>
      </ul>

      <p>The dataset we will be tearing into today is an interesting bank marketing dataset graciously hosted over at UCI. You can find it <a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">here.</a>

It is a dataset representing data collected from direct marketing campaigns of a Portuguese banking institution. The dataset consists of 41,188 rows and 21 columns which will represent our features. Clients received multiple contacts often. The question we will be trying to answer with the data is whether or not a client will subscribe a term deposit. Time to dig in!

Let’s start by discussing and importing the modules (library that allows us to use advanced functions) we’re going to need.


<ul><strong> Modules that we will use:</strong>
  <li>Numpy</li>
  <li>Pandas</li>
  <li>Matplotlib</li>
  <li>Seaborn</li>
  <li>SKlearn</li>
  <li>Operator</li>
  <li>Time</li>
</ul>
Numpy is an excellent library and it allows us to compute numerous advanced mathematical functions, work with arrays, and much more. <br />
<br />
<a style="color: #999999;" href="https://docs.scipy.org/doc/numpy/">Numpy documents.</a> <br />
<br />

Pandas is built off of the Numpy library, and allows us to work with the data in a more structured manner. Pandas allows you to use data frames, and manipulate them in various ways.<br />
<br />
<a style="color: #999999;" href="https://pandas.pydata.org/pandas-docs/stable/">Pandas documents.</a><br />
<br />
Matplotlib is our resident graphing magician. It is excellent at allowing us to visualize our data very efficiently.<br />
<br />
<a style="color: #999999;" href="https://matplotlib.org/contents.html">Matplotlib documents.</a><br />
<br />
Seaborn is great for making plots look really nice, and it also contains some advanced plots. Seaborn is based off of matplotlib.<br />
<br />
<a style="color: #999999;" href="https://seaborn.pydata.org/tutorial.html">Seaborn documents.</a><br />
<br />
Sklearn is a great package. It is very powerful and it is the library that we will use to build our models today. We will use various clustering techniques and machine learning techniques utilizing SKlearn.<br />
<br />
<a style="color: #999999;" href="http://scikit-learn.org/stable/documentation.html">SKlearn documents.</a><br />
<br />
The operator module allows us to use more efficient functions to perform our operations.<br />
<br />
<a style="color: #999999;" href="https://docs.python.org/3/library/operator.html">Operator documents.</a><br />
<br />
The time module will allow us to see how long our models are taking to computer.<br />
<br />
<a style="color: #999999;" href="https://docs.python.org/3/library/time.html">Time documents.</a><br />
<br />
Great let’s start building.<br />
<br />


<font color="#C0C0C0">
<pre><code>
<font color="#b0e0e6">import</font> numpy as np
<font color="#b0e0e6">import</font> pandas <font color="#b0e0e6">as</font> pd

<font color="#b0e0e6">import</font> matplotlib.pyplot <font color="#b0e0e6">as</font> plt
<font color="#b0e0e6">import</font> seaborn <font color="#b0e0e6">as</font> sns

<font color="#b0e0e6">import</font> sklearn
<font color="#b0e0e6">from</font> sklearn.model_selection <font color="#b0e0e6">import</font> train_test_split, cross_val_score
<font color="#b0e0e6">from</font> sklearn.decomposition <font color="#b0e0e6">import</font> PCA
<font color="#b0e0e6">from</font> sklearn.cluster <font color="#b0e0e6">import</font> KMeans, MeanShift, estimate_bandwidth
<font color="#b0e0e6">from</font> sklearn.ensemble <font color="#b0e0e6">import</font> RandomForestClassifier, GradientBoostingClassifier
<font color="#b0e0e6">from</font> sklearn.neural_network <font color="#b0e0e6">import</font> MLPClassifier
<font color="#b0e0e6">from</font> sklearn <font color="#b0e0e6">import</font> metrics, preprocessing

<font color="#b0e0e6">import</font> operator
<font color="#b0e0e6">import</font> time
<font color="#b0e0e6">from</font> datetime <font color="#b0e0e6">import</font> timedelta

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
%matplotlib inline
</pre></code>
</font>
<br />
Above you can see the imports that we’re going to use which we previously discussed. We will also set some display options that will allow us to better view the data. The %matplotlib inline allows us to view our matplotlib plots in our notebook.<br />
<br />
Let’s import our data.<br />
<br />


<font color="#C0C0C0">
<pre><code>
  df = pd.read_csv('bankmarketing.csv', delimiter=';')
  df.head()
  </pre></code>
</font>


<br />
Great our data is imported and the only modifications we had to make were, splitting the data using a semi-colon,<br />
<br />
Now let’s take a quick dive into the data and look at the shape, nulls and data types of our features:<br />
<br />

<font color="#C0C0C0">
<pre><code>
  print('Shape:\n',df.shape,'\n')
  print('Nulls:\n', df.isnull().sum(),'\n')
  print('Data types:\n', df.dtypes)
  </pre></code>
<br />
<pre><code>
<strong>Shape:</strong><br />
(41188, 21) <br />

<strong>Nulls:</strong><br />
age               0
job               0
marital           0
education         0
default           0
housing           0
loan              0
contact           0
month             0
day_of_week       0
duration          0
campaign          0
pdays             0
previous          0
poutcome          0
emp.var.rate      0
cons.price.idx    0
cons.conf.idx     0
euribor3m         0
nr.employed       0
y                 0
dtype: int64 <br />
<strong>Data types:</strong><br />
age                 int64
job                object
marital            object
education          object
default            object
housing            object
loan               object
contact            object
month              object
day_of_week        object
duration            int64
campaign            int64
pdays               int64
previous            int64
poutcome           object
emp.var.rate      float64
cons.price.idx    float64
cons.conf.idx     float64
euribor3m         float64
nr.employed       float64
y                  object
dtype: object

</pre></code>
</font>
<br />
Great so we can see that we are working with 41,000 rows and 21 features. There are also no nulls. However all of our data types are different and we need to convert all of our data to numerical. Let’s get started.
<br />
<font color="#C0C0C0">
<pre><code>
counter = 0
categorical_features = ['age', 'job', 'marital', 'education', 'default', 'housing',
                        'loan', 'contact', 'month', 'day_of_week', 'poutcome', 'y']
for category in categorical_features:
    counter = 0
    for x in df[category].unique():
        df[category] = df[category].replace(x, counter)
        counter += 1
      </font>
        </pre></code>
    <br />
    Great, we created a for loop to loop through a list of our categorical features, giving all of our data points a numerical value.
    <br />
    Time to split our data set up.  We will create our input variable and our output variable.
    <br />
    <font color="#C0C0C0">
    <pre><code>
    X = df.drop(['y'], 1)
    y = df['y']
    </pre></code>
  </font>
  <br />
  X will represent our input variables, and y will be our outcome.  <br />
  <br />
Now lets process the data. It is important that we normalize our data to give us a normal distribution which will eliminate some variance and redundancy. We will also make sure everything is numerical. Finally we will split our data in order for us to train our model on the training data, and test it on the testing data to ensure there is no overfitting going on.
<br />
<font color="#C0C0C0">
<pre><code>
columns = X.columns

X = X.apply(pd.to_numeric, errors = 'coerce')
X = preprocessing.normalize(X)
X = pd.DataFrame(X, columns=columns)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)
</pre></code>
</font>
  <br />
  Excellent. Our input variables X has been processed and is ready to fire on all cylinders. The split has also been executed successfully. Our train size is 67% of the data and our testing set is 33%.  <br />
  <br />
  Let’s start building our first neural network. We will use Sklearns MLPClassifier. We will use 5 hidden networks, and start off with the ‘relu’ activation function which is usually the best in my experience.We will time our model to determine how long it takes to fit the data. Next we will print out a cross validation score on the test data to see how well our model performed, and to determine whether or not there is any overfitting.  <br />
  <br />
  <font color="#C0C0C0">
  <pre><code>
start_time = time.monotonic()

mlp_relu = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000, 1000, 1000), activation='relu')
mlp_relu.fit(X_train, y_train)

end_time = time.monotonic()
print('Time:', timedelta(seconds=end_time - start_time))
------------------------------------------------------
Time: 0:08:13.911743
</pre></code>
</font><br />
Our data is now fit to more model. Look at the time though with 67% of the data it took over 8 minutes to fit the data.
<br />
<font color="#C0C0C0">
<pre><code>
  start_time = time.monotonic()


  print(cross_val_score(mlp_relu, X_test, y_test, cv=5))

  end_time = time.monotonic()
  print('Time:', timedelta(seconds=end_time - start_time))
  --------------------------------------------------------
  [0.89959544 0.91062891 0.91798455 0.90327326 0.9076187 ]
  Time: 0:49:32.707731
</pre></code>
</font>
<br />

Our function worked pretty well. We were able to achieve an average of 90% across our cross validation test. It looks like there wasn’t any overfitting happening. It took 38 and a half minutes to run our cross validation test on our test data, which is only 33% of the data.
<br />
Now let’s compare our relu model with a tanh model. Without further ado I give you, our tanh model:<br />

<br />
<font color="#C0C0C0">
<pre><code>
start_time = time.monotonic()

mlp_tanh = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000, 1000, 1000), activation='tanh')
mlp_tanh.fit(X_train, y_train)

end_time = time.monotonic()
print('Time:', timedelta(seconds=end_time - start_time))
--------------------------------------------------------
Time: 0:23:34.447781

</pre></code>
</font>
<br />
Our tanh model took nearly three times as longer than our relu model to fit our data. Hopefully the performance was worth the wait. Let's check it out.
<br />
<font color="#C0C0C0">
<pre><code>
  start_time = time.monotonic()

  print(cross_val_score(mlp_tanh, X_test, y_test, cv=5))

  end_time = time.monotonic()
  print('Time:', timedelta(seconds=end_time - start_time))
--------------------------------------------------------
[0.90069879 0.90805443 0.91136447 0.90511217 0.8951049 ]
Time: 0:38:22.637670
</pre></code>
</font>
<br />
It seems our relu model is a slightly better fit than our tanh model. It's not a bad thing though, since the time to fit was much less.
<br />
What about the sigmoid function, also known as logistic? Let’s see how it performs now.
<br />
<font color="#C0C0C0">
<pre><code>
  start_time = time.monotonic()

  mlp_logistic = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000, 1000, 1000), activation='logistic')
  mlp_logistic.fit(X_train, y_train)

  end_time = time.monotonic()
  print('Time:', timedelta(seconds=end_time - start_time))
  --------------------------------------------------------
  Time: 0:04:31.612431


</pre></code>
</font>
<br />
Wow! Our resident speedy Gonzales appeared. It only took four and a half minutes to fit the data using the logistic function. I’m intrigued to see how it is going to perform.
<br />
<font color="#C0C0C0">
<pre><code>
  start_time = time.monotonic()

  print(cross_val_score(mlp_logistic, X_test, y_test, cv=5))

  end_time = time.monotonic()
  print('Time:', timedelta(seconds=end_time - start_time))
--------------------------------------------------------
[0.88856197 0.88856197 0.88856197 0.88856197 0.88884799]
Time: 0:11:39.000124
</pre></code>
</font>
<br />
A fast performer here too. It did decently well, but there is a big drop off from our relu and tanh models.
<br />
Now let’s take a look at our final model. Our boosted trees classifier.
<br />
<font color="#C0C0C0">
<pre><code>
  start_time = time.monotonic()

  params = {'n_estimators': 500,
            'max_depth': 2,
            'loss': 'deviance'}

  gbr = GradientBoostingClassifier(**params)
  gbr.fit(X_train, y_train)

  end_time = time.monotonic()
  print('Time:', timedelta(seconds=end_time - start_time))
  --------------------------------------------------------
  Time: 0:00:09.370913
</pre></code>
</font>
<br />
Our boosted trees model performed extremely quickly only taking nine seconds to run, but how well did it do?
<br />
<font color="#C0C0C0">
<pre><code>
  start_time = time.monotonic()

  print(cross_val_score(gbr, X_test, y_test, cv=5))

  end_time = time.monotonic()
  print('Time:', timedelta(seconds=end_time - start_time))
--------------------------------------------------------
[0.91026113 0.91945568 0.92092681 0.91651342 0.91534781]
Time: 0:00:17.656936
</pre></code>
</font>
<br />
Clearly our boosted trees model is the most efficient. It has a higher accuracy average and performed quickly.
<br />
<br />

<strong>Discussion and conclusion:</strong><br />

All of our models performed well. Our MLP classifier models took a long time to operate. The accuracy scores are satisfactory, but the time was just totally inefficient. Our boosted trees model performed the best, though slightly, it was by far the fastest.
<br />
<br />

I believe the neural network could outperform the boosted trees model, however I don’t have the computational resources to push it to a much higher capacity. Neural networks and boosted trees are both very effective, and have a multitude of parameters that you can adjust to better fit your data, however neural networks are exceptionally computationally expensive and are hard to do much with, unless you have a nicely size server. If working with neural networks, or any computationally expensive algorithm for that matter, I recommend that you work with a smaller chunk of data initially in order to make sure that all of your programming is working correctly and efficiently, and after you have everything programmed, run all of the data through your models.
<br />
<br />
Thanks for reading my report. I hope you were able to learn something today. All questions and comments are welcome below,
<br />
<br />
Thanks again,<br />
<br />

Chris
    </font>
  </body>
</html>
